海量数据处理，其实就是基于海量数据的存储、删除、搜索等操作。所谓海量，就是数据量太大，所以导致要么无法在短时间内迅速处理，要么无法一次性装入内存。

针对时间，我们可以采用更加精妙而迅速的数据结构和算法，比如BloomFilter、Hash、堆、Bitmap等；针对空间，无非就是：大而化小，分而治之。

算法方面：

- 外排序算法（External Sorting）
- Map Reduce
- 非精确算法
- 概率算法
- 哈希算法与哈希函数（Hash Function）

数据结构方面：

- 哈希表（Hash Table）
- 堆（Heap）
- 布隆过滤器（BloomFilter）
- 位图（Bitmap）



# 一、mapreduce

![MapReduce_3UC1v8Q](img\MapReduce_3UC1v8Q.png)

为什么一定要使用Map reduce来分割文件呢，单纯的分割文件分别统计是否可行呢？
其实是不行的。单纯的将文件1丢给机器1，文件2丢给机器2，分别统计 Top K 之后再合并，这种方法是不行的。因为最高频的那一项可能分别出现在文件1和文件2，这样就相当于降低了其出现的频率，可能造成统计结果不对。

[使用 MapReduce 实现大规模稀疏矩阵相乘](http://vividfree.github.io/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/2015/11/14/large-scale-matrix-multiplication-using-mapreduce)



[PageRank算法简介及Map-Reduce实现](https://blog.csdn.net/m53931422/article/details/41745175)



# 二、topk

1、在一个整数数组中，找最大的 K 个整数，数组是离线的

也可以直接使用 Quick Select 算法（[参考资料](https://blog.csdn.net/hustyangju/article/details/25399937)），在 O(N) 的时间内找到数组的第K大的数。然后对前k大的数进行排序，时间复杂度是O(n + k log k)。

2、在一个整数数组中，找最大的 K 个整数，数组是数据流

使用堆来保存当前时刻top k的元素。我们的处理主要基于以下几点：

- 对于每一个中间时刻，前面的处理中，已经不属于top k的数据，在后续的处理中，也不可能进入top k的行列，因此没有保存的必要。
- 在已经求出前面所有数据top k项的前提下，加入一个新的数据，只需要将新数据与原来top k的数据中最小的元素比较就可以了，如果新来的元素更大，则原来top k的数据中最小的元素被剔除，将新来的元素加入top k的行列；否则保持原来top k的数据不变。
- 根据前面的分析，保存固定数量的k个数据，每次选出最小的元素，并支持添加和删除最小的元素的操作，数据结构选取最小堆最合适。

3、最高频K 项

**标准离线算法**。主要的使用了两个数据结构：哈希表和最小堆。
我们来分析一下这个算法的时空复杂度：第一步统计所有单词的出现次数，需要 O(N) 的空间和 O(N) 的时间。第二步需要 O(K) 的空间和 O(NlogK) 的时间。总的时间耗费是 O(N log K)，空间耗费是 O(N)。

4、离线数据量巨大

mapreduce 解决

1. 通过 Map 步骤，将每一个文件中的单词一个个取出，每个单词构造一个 <Word, 1> 的 Key-value 二元组，作为 Map 的输出。
2. 通过 Reduce 的步骤，每个 Reducer（Reducer是处理reduce的机器） 会处理若干个不同的 Key，在每个 Reducer 一开始初始化的时候，构建一个最小堆（如最开始我们提到的算法），Reducer 在每次 Reduce 操作的时候，输入是 key（某个 word） 和他对应的values，其实这里我们可以假设 values 就是一堆 1（事实上 Map Reduce 会帮你做一些优化，导致有可能 value 已经被加过，所以实际处理的时候，还是老老实实的把 values 加起来，而不是看一下 values 有多少个）。那么我们把所有的 values 加起来就是当前这个 key（某个 word）的出现次数。那么当我们拿到这个单词的出现次数之后，就可以在当前的 Reducer 里去和最小堆里的第K大比大小，来决定是否淘汰当前的第K大了。Reducer 在处理完他需要处理的数据之后，就输出他得到的 Top K。
3. 由于可能有多个 Reducers（跟你同时运行的机器数有关，当然一台机器也可能会运行多个Reducer），因此我们会得到多个 Top K，最后还需要从这些输出中过一遍，得到最终的 Top K。这个步骤已经在 Map Reduce 之外了，用一个单独的代码扫一遍就可以了。

5、**简化问题**：假设现在只有一台机器，内存为 1G，你有一个 1T 大小的文件，需要统计里面最高频的 K 个单词。在这个问题中，我们主要用到哈希算法来优化我们的空间效率。

1. 我们只需要先将文件扫描一次，把每个单词作为 Key，算一下他的哈希值，然后模上大概 2000 - 10000 的这样一个数。之所以取这这么一个数是因为，内存的大小是 1G，那么如果将 1T 的文件分成若干个 1G 大小的小文件的话，那么理想需要 1000 个文件。因此反之，如果你将所有的单词，分成了 1000 组的话，理想状况下，每组大概就是 1G 个不同的单词。当然这是理想状况，所以实际上处理的时候，你可以分成 2000 组比较保险。10000 组当然更保险了，但是可能就没有合理利用上内存了。实际做的时候，你可以看一下分成 2000 行不行，不行的话，再放大分组数。
2. 对于每个文件，分别导入内存进行处理，即使用我们最开始提到的标准离线算法 - 哈希表+最小堆。每一组文件得到一个 Top K。
3. 类似于 Map Reduce 一样，我们得到了若干个 Top K，我们最后把这若干个 Top K 再合并一次就好了。

6、最高频k 项的在线算法

标准在线算法，思路是：一边计数的同时，一边比较Top K

![标准在线算法](img\标准在线算法.png)

标准在线算法的时空复杂度：

- add 的时间复杂度是 O(logK) 的，因为最坏情况下，就是 pop 掉一个单词，push进去一个新的单词。由于 hashheap 的大小最多是 K，那么复杂度为 O(logK)
- topk 的时间复杂度是 O(KlogK)。
  这个算法的空间复杂度，为计数所用的哈希表的空间复杂度。为数据流中到当前时刻为止的单词总个数。

7、用损失精度换空间的方法。

在标准在线的算法的基础上改进最小的：Hash Count。

![改进标准在线](img\改进标准在线.png)

和标准在线算法相比，唯一的区别在于，我们将原本记录所有单词的出现次数的哈希表，换成了一个根据内存大小能开多大开多大的数组。这个数组的每个下标存储了“某些”单词的出现次数。我们使用了哈希函数（hashfunc），对每个单词计算他的哈希值（hashcode），将这个值模整个hashcount数组的大小得到一个下标（index），然后用这个下标对应的计数来代表这个单词的出现次数。有了这个单词的出现次数之后，再拿去 hash heap 里进行比较就可以了。



你应该马上会发现问题，如果有两个单词，他们的 hashcode % hashcount.size 的结果相同，那么他们的计数会被叠加到一起。从而导致计算结果的不精确。



事实上，根据“长尾效应”（Google 一下），在实际数据的统计中，由于 Top K 的 K 相对于整个数据流集合中的不同数据项个数 N 的关系是 K 远远小于 N，而 Top K 的这些数据项的计数又远远大于其他的数据项。因此，Top K 的 hashcode % hashcount.size 扎堆的可能性是非常非常小的。因此这个算法的精确度也就并不会太差。

**No.1 离地球最近的 K 颗星星**
给你 N 个星星的位置坐标，找到离地球最近的 M 颗星星

**No.2 最近7天的热门歌曲**

**问题描述：**
计一个听歌统计系统，返回用户 7 天内听的最多 10 首的歌

**问题分析：**
在解决这个问题之前，我们需要和面试官沟通如下的几个问题条件：

- 7天和10首歌这个数字是固定的么？有可能一会儿7天一会儿10天，一会儿10首歌一会儿8首歌么？
- 对实时性要求严格么？即，是否允许一定时间的延迟？比如一首个一分钟内被点爆，是否需要在这1分钟之内在榜单中体现出来？

澄清问题是面试中重要的一个步骤，因为上述问题的答案，稍有不同，则算法的设计，系统的设计就截然不同。
我们先做如下的合理假设：

- 7天10首歌这两个数字是固定的。
- 对实时性要求不严格，可以有1小时的误差。

**方法1: 离线算法：**
这种方法比较简单粗暴，但也非常行之有效。因为通常来说，系统都会进行一些 log。比如用户在什么时候听了什么歌曲，都会被作为一条条的log 记录下来，用于以后的大数据分析用户行为的之用。那么这个时候，我们可以每小时运行一次分析程序，来计算最近7天被听的最多的10首歌。这个分析程序则读取最近 7 天的听歌记录，用前面的 Hash + Heap 的方法进行统计即可。如果这个记录过大，需要加速的话，还可以使用 Map Reduce 来提速。

**方法2：在线算法：**
这个问题较普通的 Top K 问题的区别和难点在于，有7天这个时间窗口。这个时间窗口意味着几件事情：

- 新数据来的时候，需要丢弃对应的7天前的旧数据。
- 7天之内的数据，都应该按照某种带着时间标记的方式被保存下来，而不是只有一个计数。

在“方法1”中的算法，存在着如下一些缺点：

- 每小时都需要进行一次对前7天的数据统计，如果数据量很大，则工作量就很大，如果使用 Map Reduce 则会耗费很多计算资源。
- 如果系统的实时性要求变高，如5分钟，则该方法很有可能不奏效，因为可能5分钟无法完成对过去7天的听歌记录的统计工作。

针对这两个缺点，这里提出一种基于桶（Bucket）的统计方法：

- **聚合（Aggregate）**：将用户的同个记录，按照1小时为单位进行一次聚合（Aggregate），即整合成一个 Hash 表，Key是歌曲的id，Value是歌曲在这1小时之内被播放的次数。这种方法的好处在于，因为很多歌曲，特别是热门歌曲，是被高频率点播的，这个时候没有必要去一条一条的记录点播记录，只需要记录一个1小时的统计即可。这里每个小时就是一个桶（Bucket），比如当前时刻是 1月1日的18点，那么18点之后，19点之前的点播记录，都放在18点的这个桶里，进行聚合统计。
- **滑动窗口（Sliding Window）**：7天的话，只需要在内存中保存 7 * 24 = 144 个桶，随着时间轴的推移，旧的桶则可以被删除。每次需要获得 Top 10 的时候，则将这 144 个桶的结果进行合并即可。

这种方法的好处在于，如果我们对这个实时性的要求提高了，如提高到了5分钟，则把桶的大小缩小到5分钟即可。

**Q1**: 如果我们对内存要求很高，每个桶统计所用的哈希表太大，无法放进内存怎么办？

**A1**：这种情况下，我们必须允许一定程度的结果误差，在每个桶的局部统计中，我们可以删除那些 value 很小的 key。因为根据长尾理论，这些很冷门的歌曲，事实上占据了很大部分的内存空间，而他们最终也不会成为 Top 10 的热门歌曲。

**Q2**: 桶存在哪儿？如果是内存的话，断电了怎么办？

**A2**: 桶存同时存在内存和硬盘中。存在内存中的目的，是为了更快的计算 Top 10，存在硬盘中的目的，是断电之后可以重新快速 load 桶的内容进来。另外还有一个保险机制是，用户点播的log依然会存在数据库中，即便桶没有被存储下来，也可以通过这些点播的log重新还原每一个桶里的Hash 表。

**Q3**: 是每次用户想要查看 Top 10 的时候，都进行一次统计么？

**A3**: 不是，这个统计只会每小时进行一次。一旦统计结果获得了，就可以存在数据库里，并 cache 在内存中供用户读取。

**Q4**: 如果实时性要求高，比如需要精确到秒为单位，那样桶所起到的优化效果就很小了，这个时候如何快速获得最近7天的Top10？

**A4**: 如果是这样，则必须要有很大的内存。在 Hash + Heap 的方法中，Hash 和 Heap 都保存下所有的歌曲和其对应的点播次数。随着时间的推移，一些歌曲点播次数增加，一些歌曲的点播次数减少，然后点播次数有变化的歌曲，都在 Heap 中调整其相应的位置。然后需要获得 Top 10 时，则从 Heap 中获得。

**No.3** 访问 Baidu 次数最多的 IP
**问题描述**
给你海量的日志数据，提取出访问百度次数最多的那个 IP。
（为了简化问题，我们假设从每一行日志数据中提取 IP 的工具已经有了）
假设内存 < 4G

**问题分析**
首先问题说的是 IP 地址，IP 地址的范围转换为整数是在 0 ~ 2^32-1 的。但是问题中也给出了，内存大小 < 4G，因此如果我们需要开辟一个 2^32 的整数数组，是不够的。

**初步解决方法**
解决这种内存不够的大数据处理问题，通常的办法是：内存不够，就分批次处理。假如内存现在是 2G 的话，可以将这些日志数据分两批处理，IP地址的二进制表示中，先末尾是0的，再统计末尾是1的。各自保存下一个访问次数最多的 IP 地址，最后两者中取较大值。这种方法的缺点需要多次扫描文件。有一些网上的解答中，会说先把文件分割成多个文件之后再进行处理。虽然算法本质上没错，但是实现的时候不能这么实现，因为实现文件分割会产生“写”操作，写操作是比读操作慢很多的，而且也没有必要。所以读两次文件即可，而不需要真的把文件给拆分成小文件。

**进一步 Follow up：如何避免多次扫描**
上面这个方法很容易想到，但是缺点也很明显。就是需要多次扫描日志文件。当这个日志文件记录非常非常大的时候，每一次扫描都是很大的耗费。有没有只扫描一次就能找到最大出现次数IP的方法呢？会想我们在这一章节中学过的最高频 K 项的在线算法。事实上，这就是一个 K = 1 的特殊情况。而在之前的解决方法中，我们也提到过，可以用 Hash Count 的算法，在极小概率会损失准确性的情况下来获得空间的优化。所以那些能够节省空间的高频项统计算法都是可以解决该问题的。唯一缺点是，有可能会损失准确性，即有可能求出的最高频项不是真正的最高项，但是这个概率很低，就看实际应用场景中对准确性的要求了。

另外也可以使用升级版的 Hash Count，即布隆过滤器（Bloom Filter）来进行统计。



**No.4** 在 1B 个数中找出最小的 1M 个数

**问题描述**
Amazon：在 10 亿个数中找最小的 100 万个数。假设内存只能放下 100 万个数。

**解析**
使用一个最大堆（Max Heap），保存最小的前 100 万个数。循环每个数的过程中，和 Max Heap 的堆顶比较，看看是否能被加入最小前 100 万个数里。



# 三、布隆过滤器

布隆过滤器（Bloom Filter，简写为BF）对普通的哈希表做了进一步的改进，是一种更省空间的哈希表。当碰到内存不够的问题时，BF就是一个很好的选择。

BF一般有两个功能：

- 检测一个元素在不在一个集合中
- 统计一个元素的出现次数

在Java里面，这不就是HashSet 和 HashMap 的作用么？实际上BF能做的事情就是哈希表能做的事情，但是BF 相比哈希表，耗费更少的存储空间。既然节省了空间，同样也有一个副作用：存在 **False Positive（正误识）**。

一个更完整的BloomFilter会包含以下两个部分：

- k 个完全独立的哈希函数
- 一个很大的数组
  然后根据处理的问题的不同，BloomFilter可以分为：
- 标准型布隆过滤器（Standard Bloom Filter，简写为 SBF，对应到 Java 里的 HashSet）
- 计数型布隆过滤器（Counting Bloom Filter，简写为 CBF，对应到 Java 里的 HashMap）
  对于 SBF，其包含的大数组的类型为 Boolean 类型。对于 CBF，其包含的大数组的类型为整数类型。

Magic Number 31的意义在哪里？
其实上面的这个算法，相当于把一个字符串当做了 31 进制，然后转换为整数。一遍转换的过程中一遍对 hashsize 取模，避免溢出。

标准布隆过滤器的作用相当于一个 HashSet，即提供了这样一个数据结构，他支持如下操作：

- 在集合中加入一个元素
- 判断一个元素是否在集合中（允许 False Positive）。

其实现通常包含以下几个部分：

- 初始化：开一个足够大的 boolean 数组，初始值都是 false。
- 插入一个元素：通过 k 个哈希函数，计算出元素的 k 个哈希值，对 boolean 数组的长度取模之后，标记对应的 k 个位置上的值为 true。
- 查询一个元素：通过同样的 k 哈希函数，在 boolean 数组中取出对应的 k 个位置上的值。如果都是 true，则表示该元素可能存在，如果有一个 false，则表示一定不存在

为了更好地节省空间，可以用位运算的方式来代替boolean数组。Java中可以直接用BitSet这个结构。

Counting Bloom Filter（简写为CBF）。这个数据结构类似 Java 中的 HashMap，但只能用作计数。提供如下的几种操作：

- O(1)时间内，在集合中加入一个元素
- O(1)时间内，统计某个元素在该集合中出现的次数 - 但是可能会比实际出现次数要大一些

具体的实现步骤主要包含一下基本部分：

- 初始化：开一个足够大的 int 数组，初始值都是 0。
- 插入一个元素：通过 k 个哈希函数，计算出元素的 k 个哈希值，对 int 数组的长度取模之后，将对应的 k 个位置上的值都加一。
- 查询一个元素的出现次数：通过同样的 k 哈希函数，在 int 数组中取出对应的 k 个位置上的值。并取其中的最小值来作为该元素的出现次数预估。

# 四、外排序算法

通常采用“排序-归并”的策略，将原本的大文件，拆分为若干个小文件，小文件可以读入内存中进行排序，然后使用归并操作。

因此，外排序通常分为两个基本步骤：

- 将大文件切分为若干个个小文件，并分别使用内存排好序
- 使用K路归并算法将若干个排好序的小文件合并到一个大文件中

**第一步：文件拆分：**
根据内存的大小，尽可能多的分批次的将数据 Load 到内存中，并使用系统自带的内存排序函数（或者自己写个快速排序算法），将其排好序，并输出到一个个小文件中。比如一个文件有1T，内存有1G，那么我们就这个大文件中的内容按照 1G 的大小，分批次的导入内存，排序之后输出得到 1024 个 1G 的小文件

**第二步：K路归并排序**
在完成了大文件的拆分，并对拆分出来的小文件分别进行了排序之后，就使用K路归并算法合并排序好的文件。K路归并算法使用的是数据结构堆（Heap）来完成的，使用 Java 或者 C++ 的同学可以直接用语言自带的 PriorityQueue（C++中叫priority_queue）来代替。我们将 K 个文件中的第一个元素加入到堆里，假设数据是从小到大排序的话，那么这个堆是一个最小堆（Min Heap）。每次从堆中选出最小的元素，输出到目标结果文件中，然后如果这个元素来自第 x 个文件，则从第 x 个文件中继续读入一个新的数进来放到堆里，并重复上述操作，直到所有元素都被输出到目标结果文件中。

就是在归并的过程中，一个个从文件中读入数据，一个个输出到目标文件中操作很慢，如何优化？

确实，如果我们每个文件只读入1个元素并放入堆里的话，总共只用到了 1024 个元素，这很小，没有充分的利用好内存。另外，单个读入和单个输出的方式也不是磁盘的高效使用方式。因此我们可以为输入和输出都分别加入一个缓冲（Buffer）。假如一个元素有10个字节大小的话，1024 个元素一共 10K，1G的内存可以支持约 100K 组这样的数据，那么我们就为每个文件设置一个 100K 大小的 Buffer，每次需要从某个文件中读数据，都将这个 Buffer 装满。当然 Buffer 中的数据都用完的时候，再批量的从文件中读入。输出同理，设置一个 Buffer 来避免单个输出带来的效率缓慢。

**1、求两个超大文件中 URLs 的交集**。

给定A、B两个文件，各存放50亿个URLs，每个 URL 各占 64 字节，内存限制是 4G，让你找出A、B文件共同的 URLs？

（1）这两个文件各自没有重复

**方法1：文件拆分 Sharding（也可以叫 Partitioning）**
首先能想到的最简单的方法，肯定就是要把文件从拆分。50亿，每个 URLs 64 字节，也就是 320G 大小的文件。很显然我们不能直接全部 Load 到内存中去处理。这种内存不够的问题，通常我们的解决方法都可以是使用 hash function 来将大文件拆分为若干个小文件。比如按照hashfunc(url) % 200进行拆分的话，可以拆分成为，200 个小文件 —— 也就是如果 hashfunc(url) % 200 = 1 就把这个 url 放到 1 号文件里。每个小文件理想状况下，大小约是 1.6 G，完全可以 Load 到内存里。

这种方法的好处在于，因为我们的目标是要去重，那么那些A和B中重复的 URLs，会被hashfunc(url) % 200映射到同一个文件中。这样在这个小文件中，来自 A 和 B 的 URls 在理想状况下一共 3.2G，可以全部导入内存进入重复判断筛选出有重复的 URLs。

前面说的是理想情况，那么特殊情况下，如果 hashfunc(url) % 200 的结果比较集中，就有可能会造成不同的 URLs 在同一个文件中扎堆的情况，应该如何处理呢？
这种情况下，有一些文件的大小可能会超过 4G。对于这种情况，处理的办法是进行二次拆分，把这些仍然比较大的小文件，用一个新的 hashfunc 进行拆分：hashfunc'(url) % X。这里再拆成多少个文件，可以根据文件的实际大小来定。如果二次拆分之后还是存在很大的文件，就进行三次拆分。直到每个小文件都小于 4G。

**方法2：BloomFilter：**
既然是内存空间太少的问题，我们前面讲过了一个主要用于内存过少的情况的数据结构：BloomFilter。我们可以使用一个 4G 的 Bloom Filter，它大概包含 320 亿 个 bit。把 A 文件的 50亿 个 URLs 丢入 BF 中，然后查询 B 文件的 每个 URL 是否在 BF 里。这种方法的缺点在于，320 亿个 bit 的 BF 里存 50 亿个 URLs 实在是太满了（要考虑到BF可能会用4个哈希函数），错误率会很高。因此仍然还需需要方法1中的文件拆分来分批处理。

**方法3：外排序算法：**
将A,B文件分别拆分为80个小文件，每个小文件4G。每个文件在拆分的时候，每4G的数据在内存中做快速排序并将有序的URLs输出到小文件中。
用多路归并算法，将这160个小文件进行归并，在归并的过程中，即可知道哪些是重复的 URLs。只需将重复的 URLs 记录下来即可。

（2）如果A，B各自有重复的URLs怎么处理呢？

当 A, B 各自有重复的 URLs 的时候，比如最坏情况下，A里的50亿个URLs 全部一样。B里也是。这样采用方法1这种比较容易想到的 Sharding 方法，是不奏效的，因为所有 URLs 的 hashcode 都一样，就算换不同的 hashfunc 也一样。这种情况下，需要先对两个文件进行单独的去重，方法是每 4G 的数据，放到内存中用简单的哈希表进行去重。这样，在最坏情况下，总共 320G 的数据里，一个 URLs 最多重复 80次，则不会出现太严重的扎堆情况了。算法上唯一需要稍微改动的地方是，由于 A 存在多个重复的 URLs，所以当和 B 的 URLs 被sharding 到同一个文件里的时候，需要标记一下这个 URLs 来自哪个文件，这样才能知道是否在A和B中同时出现过。
另外，使用外排序的方法，是无需对两个文件进行单独去重的步骤的。

# 五、概率类的大数据问题

如何在数据流中等概率的取出 M 个元素

1、给你一个 Google 搜索日志记录，存有上亿挑搜索记录（Query）。这些搜索记录包含不同的语言。随机挑选出其中的 100 万条中文搜索记录。假设判断一条 Query 是不是中文的工具已经写好了。

假设你一共要挑选 N 个 Queries，设置一个 N 的 Buffer，用于存放你选中的 Queries。对于每一条飞驰而过的Query，按照如下步骤执行你的算法：

1. 如果非中文，直接跳过
2. 如果 Buffer 不满，将这条 Query 直接加入 Buffer 中
3. 如果 Buffer 满了，假设当前一共出了过 M 条中文 Queries，用一个随机函数，以 N / M 的概率来决定这条 Query 是否能被选中留下。
   3.1 如果没有选中，则跳过该 Query，继续处理下一条 Query
   3.2 如果选中了，则用一个随机函数，以 1 / N 的概率从 Buffer 中随机挑选一个 Query 来丢掉，让当前的 Query 放进去。

2、Amazon: 一个文件中有很多行，不能全部放到内存中，如何等概率的随机挑出其中的一行？

先将第一行设为候选的被选中的那一行，然后一行一行的扫描文件。假如现在是第 K 行，那么第 K 行被选中踢掉现在的候选行成为新的候选行的概率为 1/K。用一个随机函数看一下是否命中这个概率即可。命中了，就替换掉现在的候选行然后继续，没有命中就继续看下一行。

