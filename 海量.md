海量数据处理，其实就是基于海量数据的存储、删除、搜索等操作。所谓海量，就是数据量太大，所以导致要么无法在短时间内迅速处理，要么无法一次性装入内存。

针对时间，我们可以采用更加精妙而迅速的数据结构和算法，比如BloomFilter、Hash、堆、Bitmap等；针对空间，无非就是：大而化小，分而治之。

算法方面：

- 外排序算法（External Sorting）
- Map Reduce
- 非精确算法
- 概率算法
- 哈希算法与哈希函数（Hash Function）

数据结构方面：

- 哈希表（Hash Table）
- 堆（Heap）
- 布隆过滤器（BloomFilter）
- 位图（Bitmap）



1、mapreduce

![MapReduce_3UC1v8Q](img\MapReduce_3UC1v8Q.png)

为什么一定要使用Map reduce来分割文件呢，单纯的分割文件分别统计是否可行呢？
其实是不行的。单纯的将文件1丢给机器1，文件2丢给机器2，分别统计 Top K 之后再合并，这种方法是不行的。因为最高频的那一项可能分别出现在文件1和文件2，这样就相当于降低了其出现的频率，可能造成统计结果不对。

[使用 MapReduce 实现大规模稀疏矩阵相乘](http://vividfree.github.io/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/2015/11/14/large-scale-matrix-multiplication-using-mapreduce)



[PageRank算法简介及Map-Reduce实现](https://blog.csdn.net/m53931422/article/details/41745175)